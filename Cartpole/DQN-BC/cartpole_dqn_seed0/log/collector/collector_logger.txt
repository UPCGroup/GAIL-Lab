[2023-07-20 11:07:55][sample_serial_collector.py:370][INFO] collect end:
episode_count: 7
envstep_count: 99
train_sample_count: 99
avg_envstep_per_episode: 14.142857142857142
avg_sample_per_episode: 14.142857142857142
avg_envstep_per_sec: 1013.1672748822176
avg_train_sample_per_sec: 1013.1672748822176
avg_episode_per_sec: 71.6380901431871
collect_time: 0.09771338105201721
reward_mean: 14.142857142857142
reward_std: 4.763809143009448
reward_max: 24.0
reward_min: 9.0
total_envstep_count: 224
total_train_sample_count: 224
total_episode_count: 7
total_duration: 0.09771338105201721
[2023-07-20 11:07:57][sample_serial_collector.py:370][INFO] collect end:
episode_count: 8
envstep_count: 183
train_sample_count: 183
avg_envstep_per_episode: 22.875
avg_sample_per_episode: 22.875
avg_envstep_per_sec: 1530.1586974393197
avg_train_sample_per_sec: 1530.1586974393197
avg_episode_per_sec: 66.89218349461507
collect_time: 0.11959543824195862
reward_mean: 22.875
reward_std: 5.599944196150529
reward_max: 32.0
reward_min: 15.0
total_envstep_count: 384
total_train_sample_count: 384
total_episode_count: 15
total_duration: 0.21730881929397583
[2023-07-20 11:07:59][sample_serial_collector.py:370][INFO] collect end:
episode_count: 7
envstep_count: 166
train_sample_count: 166
avg_envstep_per_episode: 23.714285714285715
avg_sample_per_episode: 23.714285714285715
avg_envstep_per_sec: 1720.636449765924
avg_train_sample_per_sec: 1720.636449765924
avg_episode_per_sec: 72.55695872506908
collect_time: 0.09647592902183533
reward_mean: 23.714285714285715
reward_std: 11.707314999170215
reward_max: 46.0
reward_min: 11.0
total_envstep_count: 544
total_train_sample_count: 544
total_episode_count: 22
total_duration: 0.31378474831581116
[2023-07-20 11:08:00][sample_serial_collector.py:370][INFO] collect end:
episode_count: 5
envstep_count: 94
train_sample_count: 94
avg_envstep_per_episode: 18.8
avg_sample_per_episode: 18.8
avg_envstep_per_sec: 1760.9852934690184
avg_train_sample_per_sec: 1760.9852934690184
avg_episode_per_sec: 93.6694305036712
collect_time: 0.053379207849502563
reward_mean: 18.8
reward_std: 5.230678732248808
reward_max: 25.0
reward_min: 13.0
total_envstep_count: 704
total_train_sample_count: 704
total_episode_count: 27
total_duration: 0.3671639561653137
[2023-07-20 11:08:02][sample_serial_collector.py:370][INFO] collect end:
episode_count: 8
envstep_count: 222
train_sample_count: 222
avg_envstep_per_episode: 27.75
avg_sample_per_episode: 27.75
avg_envstep_per_sec: 1668.8982402496351
avg_train_sample_per_sec: 1668.8982402496351
avg_episode_per_sec: 60.14047712611298
collect_time: 0.13302189111709595
reward_mean: 27.75
reward_std: 12.356678356257397
reward_max: 53.0
reward_min: 12.0
total_envstep_count: 864
total_train_sample_count: 864
total_episode_count: 35
total_duration: 0.5001858472824097
[2023-07-20 11:08:04][sample_serial_collector.py:370][INFO] collect end:
episode_count: 8
envstep_count: 197
train_sample_count: 197
avg_envstep_per_episode: 24.625
avg_sample_per_episode: 24.625
avg_envstep_per_sec: 2347.3295159444347
avg_train_sample_per_sec: 2347.3295159444347
avg_episode_per_sec: 95.32302602820039
collect_time: 0.08392515778541565
reward_mean: 24.625
reward_std: 18.23415407963857
reward_max: 71.0
reward_min: 12.0
total_envstep_count: 1024
total_train_sample_count: 1024
total_episode_count: 43
total_duration: 0.5841110050678253
