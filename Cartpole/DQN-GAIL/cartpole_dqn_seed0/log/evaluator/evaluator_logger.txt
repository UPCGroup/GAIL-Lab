[2023-07-19 10:12:25][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1
[2023-07-19 10:12:25][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2
[2023-07-19 10:12:25][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3
[2023-07-19 10:12:25][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4
[2023-07-19 10:12:25][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5
[2023-07-19 10:12:25][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+---------------------+---------------+---------------+
| Name  | train_iter | ckpt_name           | episode_count | envstep_count |
+-------+------------+---------------------+---------------+---------------+
| Value | 0.000000   | iteration_0.pth.tar | 5.000000      | 55.000000     |
+-------+------------+---------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 11.000000               | 1.405829      | 39.122812           | 3.556619             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:26][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1
[2023-07-19 10:12:27][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2
[2023-07-19 10:12:27][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3
[2023-07-19 10:12:27][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4
[2023-07-19 10:12:27][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5
[2023-07-19 10:12:27][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name            | episode_count | envstep_count |
+-------+------------+----------------------+---------------+---------------+
| Value | 42.000000  | iteration_42.pth.tar | 5.000000      | 55.000000     |
+-------+------------+----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 11.000000               | 0.988885      | 55.618187           | 5.056199             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:28][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 9.0000, current episode: 1
[2023-07-19 10:12:28][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 2
[2023-07-19 10:12:28][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 3
[2023-07-19 10:12:29][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 4
[2023-07-19 10:12:29][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 4
[2023-07-19 10:12:29][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 5
[2023-07-19 10:12:29][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name            | episode_count | envstep_count |
+-------+------------+----------------------+---------------+---------------+
| Value | 84.000000  | iteration_84.pth.tar | 5.000000      | 80.000000     |
+-------+------------+----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 16.000000               | 1.249182      | 64.041885           | 4.002618             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 12.400000   | 2.244994   | 16.000000  | 10.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:30][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 1
[2023-07-19 10:12:30][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 2
[2023-07-19 10:12:30][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 8.0000, current episode: 3
[2023-07-19 10:12:30][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 3
[2023-07-19 10:12:30][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 3
[2023-07-19 10:12:31][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 19.0000, current episode: 4
[2023-07-19 10:12:31][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 18.0000, current episode: 5
[2023-07-19 10:12:31][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 126.000000 | iteration_126.pth.tar | 5.000000      | 75.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 15.000000               | 1.398320      | 53.635793           | 3.575720             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 13.000000   | 4.560702   | 19.000000  | 8.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:31][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 8.0000, current episode: 1
[2023-07-19 10:12:31][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 2
[2023-07-19 10:12:32][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 3
[2023-07-19 10:12:32][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3
[2023-07-19 10:12:32][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 16.0000, current episode: 4
[2023-07-19 10:12:32][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 16.0000, current episode: 5
[2023-07-19 10:12:32][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 168.000000 | iteration_168.pth.tar | 5.000000      | 65.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 13.000000               | 1.065687      | 60.993508           | 4.691808             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 12.200000   | 3.249615   | 16.000000  | 8.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:33][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1
[2023-07-19 10:12:33][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 2
[2023-07-19 10:12:33][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3
[2023-07-19 10:12:33][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4
[2023-07-19 10:12:34][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5
[2023-07-19 10:12:34][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 210.000000 | iteration_210.pth.tar | 5.000000      | 50.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 10.000000               | 0.999942      | 50.002885           | 5.000289             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:34][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1
[2023-07-19 10:12:34][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 2
[2023-07-19 10:12:35][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3
[2023-07-19 10:12:35][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4
[2023-07-19 10:12:35][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 4
[2023-07-19 10:12:35][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5
[2023-07-19 10:12:35][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 252.000000 | iteration_252.pth.tar | 5.000000      | 55.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 11.000000               | 1.190019      | 46.217754           | 4.201614             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:36][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0000, current episode: 1
[2023-07-19 10:12:36][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 17.0000, current episode: 2
[2023-07-19 10:12:36][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 21.0000, current episode: 3
[2023-07-19 10:12:36][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 20.0000, current episode: 4
[2023-07-19 10:12:37][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0000, current episode: 4
[2023-07-19 10:12:37][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 21.0000, current episode: 5
[2023-07-19 10:12:37][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 294.000000 | iteration_294.pth.tar | 5.000000      | 90.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 18.000000               | 1.200309      | 74.980675           | 4.165593             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 17.600000   | 4.543127   | 21.000000  | 9.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 12.0000, current episode: 1
[2023-07-19 10:12:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 13.0000, current episode: 2
[2023-07-19 10:12:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 15.0000, current episode: 3
[2023-07-19 10:12:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 17.0000, current episode: 4
[2023-07-19 10:12:38][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 5
[2023-07-19 10:12:38][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 336.000000 | iteration_336.pth.tar | 5.000000      | 75.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 15.000000               | 0.930815      | 80.574593           | 5.371640             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 14.400000   | 1.743560   | 17.000000  | 12.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:39][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 12.0000, current episode: 1
[2023-07-19 10:12:39][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 12.0000, current episode: 2
[2023-07-19 10:12:39][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 15.0000, current episode: 3
[2023-07-19 10:12:39][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 15.0000, current episode: 4
[2023-07-19 10:12:39][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 5
[2023-07-19 10:12:39][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 378.000000 | iteration_378.pth.tar | 5.000000      | 75.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 15.000000               | 0.970426      | 77.285677           | 5.152378             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 13.800000   | 1.469694   | 15.000000  | 12.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:40][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 12.0000, current episode: 1
[2023-07-19 10:12:40][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 2
[2023-07-19 10:12:41][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 17.0000, current episode: 3
[2023-07-19 10:12:41][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 17.0000, current episode: 4
[2023-07-19 10:12:41][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 17.0000, current episode: 5
[2023-07-19 10:12:41][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 420.000000 | iteration_420.pth.tar | 5.000000      | 85.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 17.000000               | 1.014984      | 83.745152           | 4.926185             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 15.200000   | 2.227106   | 17.000000  | 12.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:41][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 13.0000, current episode: 1
[2023-07-19 10:12:42][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 2
[2023-07-19 10:12:42][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3
[2023-07-19 10:12:42][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 12.0000, current episode: 4
[2023-07-19 10:12:42][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 12.0000, current episode: 5
[2023-07-19 10:12:42][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 462.000000 | iteration_462.pth.tar | 5.000000      | 55.000000     |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 11.000000               | 0.904352      | 60.817030           | 5.528821             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 11.800000   | 1.469694   | 13.000000  | 9.000000   |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:43][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 20.0000, current episode: 1
[2023-07-19 10:12:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 37.0000, current episode: 2
[2023-07-19 10:12:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 44.0000, current episode: 3
[2023-07-19 10:12:44][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 31.0000, current episode: 3
[2023-07-19 10:12:45][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 49.0000, current episode: 4
[2023-07-19 10:12:45][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 25.0000, current episode: 4
[2023-07-19 10:12:45][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 31.0000, current episode: 4
[2023-07-19 10:12:46][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 80.0000, current episode: 5
[2023-07-19 10:12:46][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 504.000000 | iteration_504.pth.tar | 5.000000      | 400.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 80.000000               | 3.193607      | 125.250232          | 1.565628             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 44.400000   | 19.489484  | 80.000000  | 25.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 16.0000, current episode: 1
[2023-07-19 10:12:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 16.0000, current episode: 1
[2023-07-19 10:12:47][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 52.0000, current episode: 2
[2023-07-19 10:12:48][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 46.0000, current episode: 3
[2023-07-19 10:12:48][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 84.0000, current episode: 4
[2023-07-19 10:12:48][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 16.0000, current episode: 4
[2023-07-19 10:12:48][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 85.0000, current episode: 5
[2023-07-19 10:12:48][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 546.000000 | iteration_546.pth.tar | 5.000000      | 270.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 54.000000               | 2.168891      | 124.487573          | 2.305325             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 56.600000   | 25.842600  | 85.000000  | 16.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:49][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 19.0000, current episode: 1
[2023-07-19 10:12:49][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 27.0000, current episode: 2
[2023-07-19 10:12:50][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 36.0000, current episode: 3
[2023-07-19 10:12:50][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 21.0000, current episode: 4
[2023-07-19 10:12:50][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 30.0000, current episode: 5
[2023-07-19 10:12:50][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 588.000000 | iteration_588.pth.tar | 5.000000      | 150.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 30.000000               | 1.376824      | 108.946357          | 3.631545             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 26.600000   | 6.151423   | 36.000000  | 19.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:51][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 28.0000, current episode: 1
[2023-07-19 10:12:51][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 26.0000, current episode: 2
[2023-07-19 10:12:52][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 29.0000, current episode: 3
[2023-07-19 10:12:52][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 27.0000, current episode: 3
[2023-07-19 10:12:52][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 49.0000, current episode: 4
[2023-07-19 10:12:52][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 26.0000, current episode: 4
[2023-07-19 10:12:53][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 27.0000, current episode: 4
[2023-07-19 10:12:53][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 29.0000, current episode: 4
[2023-07-19 10:12:53][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 26.0000, current episode: 4
[2023-07-19 10:12:54][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 49.0000, current episode: 4
[2023-07-19 10:12:54][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 27.0000, current episode: 4
[2023-07-19 10:12:54][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 29.0000, current episode: 4
[2023-07-19 10:12:55][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 26.0000, current episode: 4
[2023-07-19 10:12:55][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 27.0000, current episode: 4
[2023-07-19 10:12:56][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 26.0000, current episode: 4
[2023-07-19 10:12:56][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 29.0000, current episode: 4
[2023-07-19 10:12:56][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 134.0000, current episode: 5
[2023-07-19 10:12:56][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 630.000000 | iteration_630.pth.tar | 5.000000      | 595.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 119.000000              | 5.650832      | 105.294222          | 0.884825             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 53.000000   | 41.371488  | 134.000000 | 26.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:12:59][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 93.0000, current episode: 1
[2023-07-19 10:12:59][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 132.0000, current episode: 2
[2023-07-19 10:13:00][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 121.0000, current episode: 3
[2023-07-19 10:13:00][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 129.0000, current episode: 4
[2023-07-19 10:13:00][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 138.0000, current episode: 5
[2023-07-19 10:13:00][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 672.000000 | iteration_672.pth.tar | 5.000000      | 645.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 129.000000              | 4.001832      | 161.176181          | 1.249428             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 122.600000  | 15.781001  | 138.000000 | 93.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:13:02][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 77.0000, current episode: 1
[2023-07-19 10:13:02][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 85.0000, current episode: 2
[2023-07-19 10:13:06][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 198.0000, current episode: 3
[2023-07-19 10:13:06][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 194.0000, current episode: 4
[2023-07-19 10:13:06][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 200.0000, current episode: 5
[2023-07-19 10:13:06][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 714.000000 | iteration_714.pth.tar | 5.000000      | 980.000000    |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 196.000000              | 5.422437      | 180.730540          | 0.922095             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 150.800000  | 57.080294  | 200.000000 | 77.000000  |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:13:08][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 0 finish episode, final reward: 200.0000, current episode: 1
[2023-07-19 10:13:08][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 1 finish episode, final reward: 200.0000, current episode: 2
[2023-07-19 10:13:12][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 2 finish episode, final reward: 200.0000, current episode: 3
[2023-07-19 10:13:12][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 3 finish episode, final reward: 194.0000, current episode: 4
[2023-07-19 10:13:12][interaction_serial_evaluator.py:253][INFO] [EVALUATOR]env 4 finish episode, final reward: 200.0000, current episode: 5
[2023-07-19 10:13:12][interaction_serial_evaluator.py:279][INFO] 
+-------+------------+-----------------------+---------------+---------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count |
+-------+------------+-----------------------+---------------+---------------+
| Value | 756.000000 | iteration_756.pth.tar | 5.000000      | 1000.000000   |
+-------+------------+-----------------------+---------------+---------------+
+-------+-------------------------+---------------+---------------------+----------------------+
| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |
+-------+-------------------------+---------------+---------------------+----------------------+
| Value | 200.000000              | 5.262784      | 190.013498          | 0.950067             |
+-------+-------------------------+---------------+---------------------+----------------------+
+-------+-------------+------------+------------+------------+
| Name  | reward_mean | reward_std | reward_max | reward_min |
+-------+-------------+------------+------------+------------+
| Value | 198.800000  | 2.400000   | 200.000000 | 194.000000 |
+-------+-------------+------------+------------+------------+


[2023-07-19 10:13:12][interaction_serial_evaluator.py:303][INFO] [DI-engine serial pipeline] Current episode_return: 198.8000 is greater than stop_value: 195, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.
